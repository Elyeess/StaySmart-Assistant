{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -q openpyxl tqdm"
   ],
   "metadata": {
    "id": "T_CWYzEXI2vh"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "merged_data = pd.read_excel(\"/content/merged_data_english_only.xlsx\")"
   ],
   "metadata": {
    "id": "8jSWVtkjIz1p"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906,
     "referenced_widgets": [
      "3e6f18d762fd4f57a8ff89b9419ea9cf",
      "732a593672db400c9a793a14c4b547ed",
      "5548d0ddfe834495bf57114c158901e2",
      "e85315e9b8e748bfb217a0324fbedeaf",
      "26ae2b07cae74c519108a0e935345f1b",
      "19b16c1a5f1e4e79975ed6a36b52189c",
      "0cdb04ef5cbf48589c68faf9f0666e43",
      "3a1158e151cf40f2abb64b713339349e",
      "8d63b9570aac4d6592becb5f12eff17a",
      "0f5a09e78e5c41fc87755f6fef4b7ff8",
      "1f05816b7f1048369d59b2af2b055204",
      "b78206ab695449cca8474337329f1d44",
      "d73d24a60b634d87a59cd20363038e4e",
      "e67c732eff0d40009b009440b5f60dec",
      "b800449f1a7c46e1862fe959e325db8e",
      "4365a1e35209461c98a8ad69494af818",
      "9d290906852d4f6db9302e153bb089de",
      "84df3f4d91164f37af9e8ca1ba38a4d4",
      "f1a2e0a07c9049f2bdc3e82e2741ad43",
      "66018ba8eaaf4ee398c38547fd9640ee",
      "582b8ab1c3ba42a9bc19a6b0855cea9b",
      "7b627ed70a174bf9959ab2ad17534656",
      "39b00d2b586b4bb3862383921a0857c2",
      "382ea2923d0d4aaea5c9755e9233401b",
      "2f3866b9ba2e4800b971c9b5a36cd52a",
      "eef86457e1bb417c9789035baf3b71e2",
      "f88bdb0137594258b0a8ec77e5486cb9",
      "fabc45f6f139456294385a23b9b5a31d",
      "f004bf71b7e34f5cb510665d03848cc7",
      "e4b3a088fbdb4f628e2c77313b1bd6fe",
      "b3c61f98d270486a85d2ca9cd856cf11",
      "3283c5febbe445f58a9306b5cd32dc54",
      "aaacc1fe1d6f47f3a4ee650d65708f22",
      "f7b1f82e504f42b68aa499fd063e4420",
      "23fb9c4fc99647f3b100bb725e6a8ebf",
      "0b453e505f6642f2b58e6af344236ca5",
      "8895f2fb41974f0a88039798b07f53bb",
      "92f9817dd65847548c7ece5d48096293",
      "5c04168c55254d5e9802a1a5e4d96ed9",
      "9acc33b855c24383a79b7a4a29602626",
      "bcc95d017fe148878f1d45eae7f16752",
      "f9400db19eeb41c19b76b7307333acde",
      "95e3dfb6c83f4042a94bf9e65440f28d",
      "c2d5a198dc914ece8aac1de02ed33b5c",
      "6b3483f21fcf42998df17d848988c678",
      "c8e19ff7d13442fdbedb8d3531a63249",
      "e5f6e8565e524b5ba32f606c75bfa70b",
      "9b17de4d87834c3191f06527c2c9feff",
      "8ad70f7ec49547f39dc70c404e8a9cda",
      "a8bc7ba98d34412c859160d71b083f9b",
      "e497c6d693ac42b4805fbc8d5c442f3b",
      "75d35dffaa1c483e9fa2e2fdea494df1",
      "773e6663c7f644b69b0e488a8ff4b09e",
      "c1dec644b6d944bdbf7600248364afc6",
      "427e91b3daab447097417187646a5ee1",
      "e242adc0172341c2937292b8c25535e3",
      "ae92565bcd5d4fe3a956907c8c784c31",
      "2f2e4387b562482987d0d217fa61fdac",
      "2bd1a3c6772142e3a20881e9736840f6",
      "32dcdfd253e742dd84fcc80cf2765bf7",
      "3da65b40e5614d11971b8854e4fab672",
      "97a32f8e9ace415dbe84440d4dadba5d",
      "f63fdbed8db846fda6ca46c43aef1eef",
      "ce88b232529f4fa6bc8eafb1f4d7673d",
      "e380bc72e9374228bb5319976e32b1c9",
      "8a77335c6e23483fa96cf5897c9e12f2"
     ]
    },
    "id": "CIGMqO7NIut5",
    "outputId": "10c07aaa-d4ee-4955-f349-8fcc3df4746b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e6f18d762fd4f57a8ff89b9419ea9cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b78206ab695449cca8474337329f1d44"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39b00d2b586b4bb3862383921a0857c2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7b1f82e504f42b68aa499fd063e4420"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6b3483f21fcf42998df17d848988c678"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e242adc0172341c2937292b8c25535e3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:   0%|          | 2/6017 [00:03<2:41:44,  1.61s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (543 > 512). Running this sequence through the model will result in indexing errors\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:   0%|          | 11/6017 [00:16<2:10:24,  1.30s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:   3%|\u258e         | 189/6017 [02:14<1:00:37,  1.60it/s]Your max_length is set to 50, but your input_length is only 41. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:   4%|\u258d         | 250/6017 [02:52<55:42,  1.73it/s]Your max_length is set to 50, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:   4%|\u258d         | 266/6017 [03:02<1:00:44,  1.58it/s]Your max_length is set to 50, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:   8%|\u258a         | 471/6017 [05:15<58:05,  1.59it/s]Your max_length is set to 50, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  15%|\u2588\u258d        | 889/6017 [09:46<57:32,  1.49it/s]Your max_length is set to 50, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  16%|\u2588\u258c        | 944/6017 [10:22<56:49,  1.49it/s]Your max_length is set to 50, but your input_length is only 19. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  32%|\u2588\u2588\u2588\u258f      | 1923/6017 [21:02<47:38,  1.43it/s]Your max_length is set to 50, but your input_length is only 33. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  34%|\u2588\u2588\u2588\u258e      | 2028/6017 [22:11<41:04,  1.62it/s]Your max_length is set to 50, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  35%|\u2588\u2588\u2588\u258c      | 2113/6017 [23:06<42:04,  1.55it/s]Your max_length is set to 50, but your input_length is only 48. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  43%|\u2588\u2588\u2588\u2588\u258e     | 2584/6017 [28:12<42:28,  1.35it/s]Your max_length is set to 50, but your input_length is only 47. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  48%|\u2588\u2588\u2588\u2588\u258a     | 2895/6017 [31:32<29:37,  1.76it/s]Your max_length is set to 50, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 3463/6017 [37:37<26:22,  1.61it/s]Your max_length is set to 50, but your input_length is only 32. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=16)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 3487/6017 [37:52<26:34,  1.59it/s]Your max_length is set to 50, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 3519/6017 [38:13<23:58,  1.74it/s]Your max_length is set to 50, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 3826/6017 [41:31<24:17,  1.50it/s]Your max_length is set to 50, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 4329/6017 [46:54<17:58,  1.57it/s]Your max_length is set to 50, but your input_length is only 21. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 4370/6017 [47:20<17:16,  1.59it/s]Your max_length is set to 50, but your input_length is only 36. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=18)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 4401/6017 [47:40<16:59,  1.58it/s]Your max_length is set to 50, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 4542/6017 [49:09<13:37,  1.81it/s]Your max_length is set to 50, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 4630/6017 [50:06<14:35,  1.58it/s]Your max_length is set to 50, but your input_length is only 49. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=24)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c | 5145/6017 [55:39<09:18,  1.56it/s]Your max_length is set to 50, but your input_length is only 40. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=20)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 5393/6017 [58:18<06:24,  1.62it/s]Your max_length is set to 50, but your input_length is only 45. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 5500/6017 [59:26<05:18,  1.62it/s]Your max_length is set to 50, but your input_length is only 34. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 5927/6017 [1:03:59<00:54,  1.65it/s]Your max_length is set to 50, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 5964/6017 [1:04:23<00:32,  1.62it/s]Your max_length is set to 50, but your input_length is only 43. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=21)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 5965/6017 [1:04:23<00:29,  1.77it/s]Your max_length is set to 50, but your input_length is only 46. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=23)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 5986/6017 [1:04:37<00:19,  1.62it/s]Your max_length is set to 50, but your input_length is only 44. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=22)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 5990/6017 [1:04:39<00:16,  1.67it/s]Your max_length is set to 50, but your input_length is only 29. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "\u270d\ufe0f G\u00e9n\u00e9ration avec T5: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6017/6017 [1:04:57<00:00,  1.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Descriptions enrichies avec T5 sauvegard\u00e9es dans 'descriptions_t5.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# \ud83d\udce6 Chargement et pr\u00e9traitement\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    text = re.sub(r\"<br\\s*/?>\", \" \", str(text))\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "# \ud83e\udde0 Charger les donn\u00e9es\n",
    "df = merged_data[['description_translated', 'comments_translated']].dropna()\n",
    "\n",
    "# \ud83d\udd01 Regrouper les commentaires par description\n",
    "df_grouped = df.groupby(\"description_translated\")[\"comments_translated\"].apply(lambda x: ' '.join(x)).reset_index()\n",
    "df_grouped.columns = [\"description\", \"all_comments\"]\n",
    "\n",
    "# \ud83d\udd17 Fusionner les commentaires + description d\u2019origine comme contexte\n",
    "df_grouped[\"input_text\"] = df_grouped[\"description\"].apply(clean_text) + \" \" + df_grouped[\"all_comments\"].apply(clean_text)\n",
    "\n",
    "# \ud83d\udd27 Charger le pipeline de r\u00e9sum\u00e9 avec T5 (petit mod\u00e8le pour commencer)\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
    "\n",
    "# \ud83d\udd01 G\u00e9n\u00e9ration des descriptions avec barre de progression\n",
    "tqdm.pandas(desc=\"\u270d\ufe0f G\u00e9n\u00e9ration avec T5\")\n",
    "def t5_summarize(text):\n",
    "    prompt = \"summarize: \" + text\n",
    "    try:\n",
    "        summary = summarizer(prompt, max_length=50, min_length=10, do_sample=False)[0][\"summary_text\"]\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        return \"R\u00e9sum\u00e9 indisponible\"\n",
    "\n",
    "df_grouped[\"generated_description\"] = df_grouped[\"input_text\"].progress_apply(t5_summarize)\n",
    "\n",
    "# \ud83d\udcbe Sauvegarde\n",
    "df_grouped.to_excel(\"descriptions_t5.xlsx\", index=False)\n",
    "print(\"\u2705 Descriptions enrichies avec T5 sauvegard\u00e9es dans 'descriptions_t5.xlsx'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "1gsh4SBKKFmT"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}